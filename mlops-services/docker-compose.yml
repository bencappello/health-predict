version: '3.8'

networks:
  mlops_network:
    driver: bridge
  minikube:
    external: true

services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflowdb
      - POSTGRES_MULTIPLE_DATABASES=airflowdb,mlflowdb
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres-init:/docker-entrypoint-initdb.d # For initial DB setup scripts
    ports:
      - "5432:5432"
    networks:
      - mlops_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    image: mlops-services-airflow-init
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: mlops-services-airflow-init-1
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW_UID=50000
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflowdb
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
      - AIRFLOW__CORE__REMOTE_LOGGING=False
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - KUBECONFIG=/home/airflow/.kube/config
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    command: >
      bash -c "
        echo 'Waiting for PostgreSQL to be ready...';
        while ! pg_isready -U $$POSTGRES_USER -h postgres; do
          echo 'PostgreSQL is not ready yet...';
          sleep 1;
        done;
        echo 'PostgreSQL is ready!';
        airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname User --role Admin --email admin@example.com"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      # - ./config:/opt/airflow/config # If you have custom airflow.cfg
    networks:
      - mlops_network

  airflow-webserver:
    image: mlops-services-airflow-webserver
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: mlops-services-airflow-webserver-1
    restart: always
    depends_on:
      airflow-scheduler:
        condition: service_started
      postgres:
        condition: service_healthy
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW_UID=50000
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflowdb
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}
      - EC2_PRIVATE_IP=${EC2_PRIVATE_IP}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
      - AIRFLOW__CORE__REMOTE_LOGGING=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - KUBECONFIG=/home/airflow/.kube/config
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      # Drift monitoring environment variables
      - DRIFT_MONITORING_EXPERIMENT=${DRIFT_MONITORING_EXPERIMENT}
      - DRIFT_REPORTS_S3_PREFIX=${DRIFT_REPORTS_S3_PREFIX}
      - DRIFT_BATCH_DATA_S3_PREFIX=${DRIFT_BATCH_DATA_S3_PREFIX}
      - DRIFT_REFERENCE_DATA_S3_PREFIX=${DRIFT_REFERENCE_DATA_S3_PREFIX}
      - DRIFT_THRESHOLD_MINOR=${DRIFT_THRESHOLD_MINOR}
      - DRIFT_THRESHOLD_MODERATE=${DRIFT_THRESHOLD_MODERATE}
      - DRIFT_THRESHOLD_MAJOR=${DRIFT_THRESHOLD_MAJOR}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../scripts:/home/jovyan/work/scripts
      - ../src:/home/jovyan/work/src
    command: airflow webserver
    networks:
      - mlops_network

  airflow-scheduler:
    image: mlops-services-airflow-scheduler
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: mlops-services-airflow-scheduler-1
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW_UID=50000
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflowdb
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}
      - EC2_PRIVATE_IP=${EC2_PRIVATE_IP}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
      - AIRFLOW__CORE__REMOTE_LOGGING=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=0
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=5
      - AIRFLOW__CELERY__BROKER_URL=
      - AIRFLOW__CELERY__RESULT_BACKEND=
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - KUBECONFIG=/home/airflow/.kube/config
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      # Drift monitoring environment variables
      - DRIFT_MONITORING_EXPERIMENT=${DRIFT_MONITORING_EXPERIMENT}
      - DRIFT_REPORTS_S3_PREFIX=${DRIFT_REPORTS_S3_PREFIX}
      - DRIFT_BATCH_DATA_S3_PREFIX=${DRIFT_BATCH_DATA_S3_PREFIX}
      - DRIFT_REFERENCE_DATA_S3_PREFIX=${DRIFT_REFERENCE_DATA_S3_PREFIX}
      - DRIFT_THRESHOLD_MINOR=${DRIFT_THRESHOLD_MINOR}
      - DRIFT_THRESHOLD_MODERATE=${DRIFT_THRESHOLD_MODERATE}
      - DRIFT_THRESHOLD_MAJOR=${DRIFT_THRESHOLD_MAJOR}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../scripts:/opt/airflow/scripts
      - ../src:/opt/airflow/src
      - /var/run/docker.sock:/var/run/docker.sock
      - /home/ubuntu/health-predict:/home/ubuntu/health-predict
      - /home/ubuntu/.kube/config_flattened:/home/airflow/.kube/config:ro
      - /home/ubuntu/.minikube:/home/airflow/.minikube:ro
    command: airflow scheduler
    networks:
      - mlops_network
      - minikube

  mlflow:
    build:
      context: .
      dockerfile: Dockerfile.mlflow
    restart: always
    depends_on:
      - postgres
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql+psycopg2://airflow:airflow@postgres:5432/mlflowdb
    command: >
      --host 0.0.0.0
      --port 5000
      --backend-store-uri postgresql+psycopg2://airflow:airflow@postgres:5432/mlflowdb
      --default-artifact-root s3://health-predict-mlops-f9ac6509/mlflow-artifacts/
    networks:
      - mlops_network

  jupyterlab:
    # image: jupyter/scipy-notebook:latest  # Reverted to pre-built image
    build:
      context: ../
      dockerfile: mlops-services/Dockerfile.jupyterlab
    container_name: jupyterlab
    ports:
      - "8888:8888"
    volumes:
      - ../:/home/jovyan/work # Mount the whole project
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      # Add any other necessary env vars like GRANT_SUDO=yes if needed for package installs within notebook
    working_dir: /home/jovyan/work # Start in the project directory
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.ip='0.0.0.0' # Example command
    networks:
      - mlops_network
    depends_on:
      - mlflow
      - postgres

volumes:
  pgdata:

# Removed the old networks alias
# networks:
#  default:
#    name: mlops_network 