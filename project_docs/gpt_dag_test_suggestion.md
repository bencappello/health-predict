Thanks for the detailed setup and logs. I’ll investigate known networking issues involving Airflow DAG tasks (specifically using PythonOperator), Docker Compose, and Minikube with the Docker driver on EC2—especially where socket connections fail but `curl` works from the same container. I’ll look for documentation, community posts, and best practices addressing these sandboxing or container process network isolation problems.

I’ll report back with relevant insights, diagnostics, and recommended workarounds or fixes.


# Networking Issues: Airflow (Docker) to FastAPI on Minikube NodePort

## Docker & Minikube Networking Challenges

When running Minikube with the Docker driver, the Kubernetes node is itself a Docker container on an isolated bridge network (usually `minikube` network with subnet `192.168.49.0/24`). The NodePort service IP (e.g. **192.168.49.2**) is only reachable on that internal network by default. Docker’s networking model **isolates containers on different networks**, which means your Airflow container (on Docker Compose’s default network) can’t directly communicate with the Minikube container’s network by default. On Mac/Windows, this isolation is strict – the Minikube Node IP isn’t reachable without a tunnel. On Linux, the Minikube Node IP is reachable *from the host* (no tunnel required), but **not automatically from other containers** due to Docker’s default iptables rules and network isolation. In short, Airflow running in one container can hit a “network barrier” when trying to reach a service on Minikube’s IP. This often manifests as timeouts or connection refusals from inside the container.

Additionally, **firewall rules or missing routes** can cause connection issues. For example, if the host’s firewall blocks NodePort range 30000–32767, connections will be refused. There’s also a known quirk on some Linux setups where the Docker `docker0` interface being down can prevent access to Minikube’s IP – simply bringing `docker0` up allowed 192.168.49.2 to become reachable in one case. These low-level issues can contribute to “Connection refused” errors even if the service is running.

## PythonOperator vs. Manual `curl` – Why the Discrepancy?

It’s perplexing that **manual `curl` inside the Airflow container succeeds while Python code (`requests.get()` or low-level sockets) fails**. The root cause is likely not the Python libraries themselves, but the network context and environment in which the Airflow task runs. In a Docker Compose setup, opening a shell in the container and curling the Minikube NodePort might work **if the container’s outgoing NAT finds a route to 192.168.49.2**. This could happen when the host bridges the networks and NATs the traffic. However, the Airflow task’s Python process could still fail if it hits any of these subtle issues:

* **Network Isolation**: If the container truly isn’t connected to the Minikube network, the traffic might be dropping. In some cases, an isolated container trying to reach another container’s IP will just hang or get refused – indicating it hit a network wall. The manual test might have been succeeding under specific conditions (e.g. after a previous `minikube service --url` tunnel, or if the container was temporarily on the same network). But the PythonOperator subprocess has no special network privileges – it runs in the same container namespace. So if one process can reach, normally all should. This suggests the issue might not be the code but network readiness or environment.

* **Timing / Service Readiness**: A common cause of ECONNREFUSED is that the target service isn’t ready or listening. If the Airflow task executed *before* the FastAPI service was fully up (or before the NodePort was allocated and accepting connections), it would get `[Errno 111] Connection refused`. Later, when you manually curl, the service is live, so it works. In Kubernetes, a NodePort is opened by kube-proxy. If no pod is serving yet, the node might refuse connections on that port. Ensure your DAG waits for the FastAPI service to be up (you might add a retry or a sensor check). This timing issue can make it seem like “Python can’t connect but curl can,” when it’s really that the second try happened after the service became reachable.

* **Environment Differences (Proxy Settings)**: Another subtle factor can be proxy environment variables. Inside some containers (or cloud environments), `HTTP_PROXY`/`HTTPS_PROXY` might be set. The `requests` library will by default attempt to use these proxies. If, for example, your environment requires using a proxy for external HTTP and hasn’t listed the Minikube IP in `NO_PROXY`, then `requests.get("http://192.168.49.2:port")` could be trying to route through a proxy and failing (leading to a refused connection from the proxy or no route). Curl might have been invoked without proxy, or the environment for the interactive shell had `NO_PROXY` configured, whereas the Airflow worker process environment did not. It’s worth checking if `NO_PROXY` includes `192.168.49.0/24` in the Airflow container’s environment. Ensuring that internal addresses are excluded from proxies can resolve cases where Python gets refused while curl works.

In summary, the PythonOperator task itself doesn’t block networking – the issue is likely the **Docker networking setup and possibly timing or env config**, not the Python code. The “\[Errno 111] Connection refused” means the TCP SYN reached something that sent an RST back (no listener), indicating either hitting the wrong place or hitting it too early.

## Solutions and Workarounds for Connectivity

To fix or work around these issues, consider the following approaches:

* **Connect the Docker Networks:** The most direct fix on Linux is to attach the Airflow container to Minikube’s network so they can communicate. For example, you can run: `docker network connect minikube <airflow_container_name>`. This command will add your Airflow (Docker Compose) container to the `minikube` bridge network. Once that’s done, both containers share a network and the Airflow container can reach services on 192.168.49.2:*NodePort* directly. A community example showed this technique to connect a Jenkins container to Minikube’s Docker driver network; after connecting networks, `curl https://192.168.49.2:8443/...` from Jenkins succeeded. You can do the same for Airflow. (In a `docker-compose.yml`, you might define an external network named “minikube” and attach the Airflow service to it, though be careful that the Minikube network is not marked internal/isolated – by default it’s not internal.)

* **Use Host Networking or Host Ports:** Running Airflow with `network_mode: "host"` in Docker Compose would put the Airflow process on the host’s network stack, meaning it can reach the Minikube node IP like any host process. This can solve connectivity (since from the host, `192.168.49.2:NodePort` is reachable), but it has downsides – e.g. port conflicts and less isolation. A more controlled variant is to expose the Minikube service on the host and have the container talk to the host. For instance, Minikube offers a **service tunnel** for NodePorts: running `minikube service <service-name> --url` will open a tunnel and print a URL like `http://127.0.0.1:XXXXX`. While that tunnel process runs, the service is reachable on a localhost port on the **host**. Your Airflow container could then connect to the host’s IP/port. On Linux, the special DNS name `host.docker.internal` (with Docker 20.04+ or via extra\_hosts) lets a container refer to the host. You could configure Airflow to call `http://host.docker.internal:XXXXX`. This way, the PythonOperator task connects to the host’s forwarded port, which in turn hits the Minikube service. This method has been used as a workaround on Mac/Windows where direct NodePort access is impossible, and it works on Linux as well – essentially outsourcing the network bridge to the host’s loopback interface.

* **Leverage Minikube Tunnels or Port-Forward:** If modifying Docker networks is undesirable (or not possible due to permissions), you can use Kubernetes port-forwarding or Minikube’s tunnel for LoadBalancer services. For example, you could change the FastAPI Service to type `LoadBalancer` and run `minikube tunnel` – this will expose a service IP on your host network. Minikube tunnel requires running as root on the host, but it will route an external IP (or 127.0.0.1 for NodePort on some setups) to your service. Alternatively, use `kubectl port-forward svc/your-service <hostPort>:<servicePort>` from the host or even from within the Airflow container (as a subprocess) before making the request. This sets up a direct pipe. The downside is needing to manage the lifecycle of the port-forward (it’s temporary). Still, for testing, this guarantees the Airflow task can reach the service at `localhost:<hostPort>`.

* **Ensure Firewall & System Settings**: On the EC2 host, double-check that nothing is blocking the traffic. If Ubuntu’s UFW is enabled, open the NodePort range: e.g. `sudo ufw allow 30000:32767/tcp`. Since the FastAPI service port is dynamic, allowing the whole NodePort range (or at least the specific port you see) is needed. Also verify the AWS security group if you ever attempt to access it externally (though for internal container-to-container traffic on the same host, the security group isn’t involved). As mentioned, ensure the `docker0` or relevant bridge interfaces are up on the host – although your curl test suggests they are. These checks prevent external factors from causing connection refusal.

* **No-Proxy for Minikube IP**: If your environment variables include a proxy, add `192.168.49.0/24` (or at least the Minikube IP) to the `NO_PROXY` list. This ensures Python requests or other libraries don’t attempt to proxy the connection. In an Airflow Docker context, you can set this in the Compose file environment. This step has resolved “\[Errno 111] refused” issues in cases where direct connections were mistakenly going through a proxy.

* **Minikube `--ports` Feature**: Newer versions of Minikube have a `--ports` flag that can publish NodePorts to the host automatically on startup. For example, `minikube start --driver=docker --ports=30080:30080` would map the Minikube container’s port 30080 to the host’s port 30080 (similar to Docker’s `-p`). This is experimental and needs you to know the NodePort in advance. If you can allocate a fixed NodePort for FastAPI, you might start minikube with `--ports=192.168.49.2:<port>:<port>` (the syntax can specify host IP). Some users have done this to expose services on `127.0.0.1` for local access. It essentially removes the need for a separate tunnel process. This could be useful on an EC2 (you’d then access via host localhost or host IP from the container, after adding host host.docker.internal mapping as needed).

## Best Practices for Airflow ↔ Minikube Integration

Given the complexity, here are some best practices and recommendations:

* **Prefer Unified Environments for Integration Testing:** If possible, run Airflow inside the Kubernetes cluster (e.g., using the KubernetesExecutor or via an Airflow Helm chart on Minikube) for integration tests. This avoids all the host-container networking gymnastics – Airflow tasks could directly call the service via its ClusterIP or service DNS. However, this may be heavy for simple testing, so if you keep Airflow external, be ready to manage networking.

* **Document and Automate Network Setup:** If you choose to connect the Docker networks, incorporate that into your deployment scripts. For example, after `docker-compose up`, your script can automatically do `docker network connect minikube airflow_scheduler_1` (and webserver if needed). The Medium community report on Jenkins noted that after connecting networks, cross-communication worked instantly. Automating this ensures developers don’t forget the step. Alternatively, create the Docker Compose network to be the same as Minikube’s network (if you can instruct Minikube to use a pre-existing network). This might be tricky (Minikube expects its own subnet), so manual connect is often simpler.

* **Use Ingress for more realistic access:** For a FastAPI service, you might simulate production by using an Ingress in Minikube. Enable Minikube’s ingress addon and expose the service via an ingress on a host URL. Then run `minikube tunnel` – this will give you a stable `127.0.0.1` (or external IP) for the ingress controller. Your Airflow container can hit `http://<ingress-host>` (which actually resolves to host and goes to Minikube). This approach uses HTTP routing instead of raw NodePorts and might integrate better if your Airflow calls services by name/host. Just remember to add an `/etc/hosts` entry or use `curl --resolve` if using a fake domain.

* **Verify Service Availability in DAGs:** Implement robust error handling in the Airflow DAG tasks. For example, have the PythonOperator try a connection multiple times with a short delay (simple retry loop) or use an Airflow Sensor to poll the FastAPI health endpoint before running the main task. This covers the case where Kubernetes has created the NodePort but your FastAPI pod isn’t fully ready. It can save you from transient “Connection refused” on first attempt.

* **Consider Alternatives to Minikube’s Docker Driver:** If networking continues to be painful, you could consider running Minikube with the `none` driver (i.e., directly on the host VM) on this EC2 instance. In that mode, Kubernetes uses the host’s network directly, so NodePorts would listen on the host’s IP address. Your Airflow container could then simply connect to `<host-private-IP>:NodePort` as it would any external service. This removes Docker-in-Docker networking issues (at the cost of running all k8s components on the host). Kind or K3d (K3s in Docker) are other alternatives – for example, k3d by default exposes an API LoadBalancer on `localhost`, and you can easily forward additional ports. The trade-off is complexity vs. convenience – for a long-lived dev setup, solving the network connectivity as above is fine.

**In conclusion**, the errors you’re seeing are a known side-effect of mixing Dockerized apps with a Dockerized Minikube. The NodePort service is reachable from the host but not automatically from an unrelated container without extra configuration. The community has encountered similar issues, for example needing to connect Jenkins to Minikube’s network to reach the Kubernetes API. The recommended solutions are to **bridge that network gap** – either by literally connecting the networks or by forwarding the service to a common interface (like localhost) that both can access. By applying these fixes (ensuring proper network routing, no proxy interference, and waiting for service readiness), your Airflow task should be able to successfully `requests.get()` the FastAPI endpoint.

**Sources:**

* Minikube documentation – NodePort access and Docker-driver limitations
* Y. Toyoda, *“Connecting Minikube and Jenkins in Docker: Solving Network Isolation Issues”* – example of container network isolation and solution via `docker network connect`
* Stack Overflow – NodePort connection refused causes (firewall/UFW)
* Server Fault – Linux Docker driver NodePort unreachable until docker0 brought up
* Kubernetes GH Issue #11193 – user workaround using `minikube service --url` to expose NodePort on localhost
* PythonAnywhere FAQ – explanation that Errno 111 can indicate proxy issues with direct connections
